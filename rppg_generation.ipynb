{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"rppg_generation.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"7O1pJFecFpQQ"},"source":["!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OiKGa-6fF1Wz"},"source":["!pip install 'git+https://github.com/facebookresearch/pytorch3d.git'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gifUD7h1PIe-"},"source":["# Library"]},{"cell_type":"code","metadata":{"id":"WEUJ4JtbF4FQ"},"source":["import time, os, sys, math\n","import cv2\n","import numpy as np\n","from time import time\n","from scipy.io import savemat\n","import argparse\n","import imageio\n","from skimage.transform import rescale\n","import torch\n","import matplotlib.pyplot as plt\n","import h5py \n","from pathlib import Path\n","import pandas as pd \n","\n","# sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n","from decalib.deca import DECA\n","from decalib.datasets import datasets \n","from decalib.utils import util\n","from decalib.utils.rotation_converter import batch_euler2axis, deg2rad\n","from decalib.utils.config import cfg as deca_cfg\n","\n","import os, sys\n","import torch\n","import torchvision\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","import numpy as np\n","from time import time\n","from skimage.io import imread\n","import cv2\n","import pickle\n","sys.path.insert(0, os.path.join(os.getcwd(), 'decalib'))\n","from utils.renderer import SRenderY_modified\n","# from utils.renderer import SRenderY\n","from models.encoders import ResnetEncoder\n","from models.FLAME import FLAME, FLAMETex\n","from models.decoders import Generator\n","from utils import util\n","from utils.rotation_converter import batch_euler2axis\n","from datasets import datasets\n","from utils.config import cfg\n","torch.backends.cudnn.benchmark = True\n","\n","import scipy.io as io\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","import torch.utils.data as utils\n","from torch.utils.data import Dataset, TensorDataset\n","import torchvision.transforms as transforms\n","\n","import argparse\n","from matplotlib import cm\n","from sklearn.decomposition import PCA"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uXY3rnQ3dn-k"},"source":["# Helper"]},{"cell_type":"code","metadata":{"id":"kSsQBZj_doFa"},"source":["def show_img(img):\n","  plt.imshow(img)\n","  plt.show()\n","\n","def show_img_final(img, savename):\n","  plt.imshow(img)\n","  plt.axis('off')\n","  plt.savefig(savename, bbox_inches='tight')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3x4EOXqaOaFK"},"source":["# DECA "]},{"cell_type":"code","metadata":{"id":"vGCCGGaog-vH"},"source":["class DECA(object):\n","    def __init__(self, config=None, device='cuda'):\n","        if config is None:\n","            self.cfg = cfg\n","        else:\n","            self.cfg = config\n","        self.device = device\n","        self.image_size = self.cfg.dataset.image_size\n","        self.uv_size = self.cfg.model.uv_size\n","\n","        self._create_model(self.cfg.model)\n","        self._setup_renderer(self.cfg.model)\n","\n","    def _setup_renderer(self, model_cfg):\n","        # self.render = SRenderY(self.image_size, obj_filename=model_cfg.topology_path, uv_size=model_cfg.uv_size).to(self.device)\n","        self.render = SRenderY_modified(self.image_size, obj_filename=model_cfg.topology_path, uv_size=model_cfg.uv_size).to(self.device)\n","        # face mask for rendering details\n","        mask = imread(model_cfg.face_eye_mask_path).astype(np.float32)/255.; mask = torch.from_numpy(mask[:,:,0])[None,None,:,:].contiguous()\n","        self.uv_face_eye_mask = F.interpolate(mask, [model_cfg.uv_size, model_cfg.uv_size]).to(self.device)\n","        mask = imread(model_cfg.face_mask_path).astype(np.float32)/255.; mask = torch.from_numpy(mask[:,:,0])[None,None,:,:].contiguous()\n","        self.uv_face_mask = F.interpolate(mask, [model_cfg.uv_size, model_cfg.uv_size]).to(self.device)\n","        # displacement correction\n","        fixed_dis = np.load(model_cfg.fixed_displacement_path)\n","        self.fixed_uv_dis = torch.tensor(fixed_dis).float().to(self.device)\n","        # mean texture\n","        mean_texture = imread(model_cfg.mean_tex_path).astype(np.float32)/255.; mean_texture = torch.from_numpy(mean_texture.transpose(2,0,1))[None,:,:,:].contiguous()\n","        self.mean_texture = F.interpolate(mean_texture, [model_cfg.uv_size, model_cfg.uv_size]).to(self.device)\n","        # dense mesh template, for save detail mesh\n","        self.dense_template = np.load(model_cfg.dense_template_path, allow_pickle=True, encoding='latin1').item()\n","\n","    def _create_model(self, model_cfg):\n","        # set up parameters\n","        self.n_param = model_cfg.n_shape+model_cfg.n_tex+model_cfg.n_exp+model_cfg.n_pose+model_cfg.n_cam+model_cfg.n_light\n","        self.n_detail = model_cfg.n_detail\n","        self.n_cond = model_cfg.n_exp + 3 # exp + jaw pose\n","        self.num_list = [model_cfg.n_shape, model_cfg.n_tex, model_cfg.n_exp, model_cfg.n_pose, model_cfg.n_cam, model_cfg.n_light]\n","        self.param_dict = {i:model_cfg.get('n_' + i) for i in model_cfg.param_list}\n","\n","        # encoders\n","        self.E_flame = ResnetEncoder(outsize=self.n_param).to(self.device) \n","        self.E_detail = ResnetEncoder(outsize=self.n_detail).to(self.device)\n","        # decoders\n","        self.flame = FLAME(model_cfg).to(self.device)\n","        if model_cfg.use_tex:\n","            self.flametex = FLAMETex(model_cfg).to(self.device)\n","        self.D_detail = Generator(latent_dim=self.n_detail+self.n_cond, out_channels=1, out_scale=model_cfg.max_z, sample_mode = 'bilinear').to(self.device)\n","        # resume model\n","        model_path = self.cfg.pretrained_modelpath\n","        if os.path.exists(model_path):\n","            print(f'trained model found. load {model_path}')\n","            checkpoint = torch.load(model_path)\n","            self.checkpoint = checkpoint\n","            util.copy_state_dict(self.E_flame.state_dict(), checkpoint['E_flame'])\n","            util.copy_state_dict(self.E_detail.state_dict(), checkpoint['E_detail'])\n","            util.copy_state_dict(self.D_detail.state_dict(), checkpoint['D_detail'])\n","        else:\n","            print(f'please check model path: {model_path}')\n","            exit()\n","        # eval mode\n","        self.E_flame.eval()\n","        self.E_detail.eval()\n","        self.D_detail.eval()\n","\n","    def decompose_code(self, code, num_dict):\n","        ''' Convert a flattened parameter vector to a dictionary of parameters\n","        code_dict.keys() = ['shape', 'tex', 'exp', 'pose', 'cam', 'light']\n","        '''\n","        code_dict = {}\n","        start = 0\n","        for key in num_dict:\n","            end = start+int(num_dict[key])\n","            code_dict[key] = code[:, start:end]\n","            start = end\n","            if key == 'light':\n","                code_dict[key] = code_dict[key].reshape(code_dict[key].shape[0], 9, 3)\n","        return code_dict\n","\n","    def displacement2normal(self, uv_z, coarse_verts, coarse_normals):\n","        ''' Convert displacement map into detail normal map\n","        '''\n","        batch_size = uv_z.shape[0]\n","        uv_coarse_vertices = self.render.world2uv(coarse_verts).detach()\n","        uv_coarse_normals = self.render.world2uv(coarse_normals).detach()\n","    \n","        uv_z = uv_z*self.uv_face_eye_mask\n","        uv_detail_vertices = uv_coarse_vertices + uv_z*uv_coarse_normals + self.fixed_uv_dis[None,None,:,:]*uv_coarse_normals.detach()\n","        dense_vertices = uv_detail_vertices.permute(0,2,3,1).reshape([batch_size, -1, 3])\n","        uv_detail_normals = util.vertex_normals(dense_vertices, self.render.dense_faces.expand(batch_size, -1, -1))\n","        uv_detail_normals = uv_detail_normals.reshape([batch_size, uv_coarse_vertices.shape[2], uv_coarse_vertices.shape[3], 3]).permute(0,3,1,2)\n","        return uv_detail_normals\n","\n","    def displacement2vertex(self, uv_z, coarse_verts, coarse_normals):\n","        ''' Convert displacement map into detail vertices\n","        '''\n","        batch_size = uv_z.shape[0]\n","        uv_coarse_vertices = self.render.world2uv(coarse_verts).detach()\n","        uv_coarse_normals = self.render.world2uv(coarse_normals).detach()\n","    \n","        uv_z = uv_z*self.uv_face_eye_mask\n","        uv_detail_vertices = uv_coarse_vertices + uv_z*uv_coarse_normals + self.fixed_uv_dis[None,None,:,:]*uv_coarse_normals.detach()\n","        dense_vertices = uv_detail_vertices.permute(0,2,3,1).reshape([batch_size, -1, 3])\n","        # uv_detail_normals = util.vertex_normals(dense_vertices, self.render.dense_faces.expand(batch_size, -1, -1))\n","        # uv_detail_normals = uv_detail_normals.reshape([batch_size, uv_coarse_vertices.shape[2], uv_coarse_vertices.shape[3], 3]).permute(0,3,1,2)\n","        detail_faces =  self.render.dense_faces\n","        return dense_vertices, detail_faces\n","\n","    def visofp(self, normals):\n","        ''' visibility of keypoints, based on the normal direction\n","        '''\n","        normals68 = self.flame.seletec_3d68(normals)\n","        vis68 = (normals68[:,:,2:] < 0.1).float()\n","        return vis68\n","\n","    @torch.no_grad()\n","    def encode(self, images):\n","        batch_size = images.shape[0]\n","        parameters = self.E_flame(images)\n","        detailcode = self.E_detail(images)\n","        codedict = self.decompose_code(parameters, self.param_dict)\n","        codedict['detail'] = detailcode\n","        codedict['images'] = images\n","        return codedict\n","\n","    @torch.no_grad()\n","    def decode(self, codedict):\n","        images = codedict['images']\n","        batch_size = images.shape[0]\n","        \n","        ## decode\n","        verts, landmarks2d, landmarks3d = self.flame(shape_params=codedict['shape'], expression_params=codedict['exp'], pose_params=codedict['pose'])\n","        uv_z = self.D_detail(torch.cat([codedict['pose'][:,3:], codedict['exp'], codedict['detail']], dim=1))\n","        if self.cfg.model.use_tex:\n","            albedo = self.flametex(codedict['tex'])\n","        else:\n","            albedo = torch.zeros([batch_size, 3, self.uv_size, self.uv_size], device=images.device) \n","        ## projection\n","        landmarks2d = util.batch_orth_proj(landmarks2d, codedict['cam'])[:,:,:2]; landmarks2d[:,:,1:] = -landmarks2d[:,:,1:]; landmarks2d = landmarks2d*self.image_size/2 + self.image_size/2\n","        landmarks3d = util.batch_orth_proj(landmarks3d, codedict['cam']); landmarks3d[:,:,1:] = -landmarks3d[:,:,1:]; landmarks3d = landmarks3d*self.image_size/2 + self.image_size/2\n","        trans_verts = util.batch_orth_proj(verts, codedict['cam']); trans_verts[:,:,1:] = -trans_verts[:,:,1:]\n","        \n","        ## rendering\n","        ops = self.render(verts, trans_verts, albedo, codedict['light'])\n","        uv_detail_normals = self.displacement2normal(uv_z, verts, ops['normals'])\n","        uv_shading = self.render.add_SHlight(uv_detail_normals, codedict['light'])\n","        uv_texture = albedo*uv_shading\n","\n","        landmarks3d_vis = self.visofp(ops['transformed_normals'])\n","        landmarks3d = torch.cat([landmarks3d, landmarks3d_vis], dim=2)\n","\n","        ## render shape\n","        shape_images = self.render.render_shape(verts, trans_verts, images=ops['albedo_images'], lights=codedict['light'])\n","        # shape_images = self.render.render_shape(verts, trans_verts)\n","\n","        detail_normal_images = F.grid_sample(uv_detail_normals, ops['grid'], align_corners=False)*ops['alpha_images']\n","        # shape_detail_images = self.render.render_shape(verts, trans_verts, detail_normal_images=detail_normal_images)\n","        shape_detail_images = self.render.render_shape(verts, trans_verts, images=ops['albedo_images'], detail_normal_images=detail_normal_images, lights=codedict['light'])\n","        \n","        ## extract texture\n","        ## TODO: current resolution 256x256, support higher resolution, and add visibility\n","        uv_pverts = self.render.world2uv(trans_verts)\n","        uv_gt = F.grid_sample(images, uv_pverts.permute(0,2,3,1)[:,:,:,:2], mode='bilinear')\n","        if self.cfg.model.use_tex:\n","            ## TODO: poisson blending should give better-looking results\n","            uv_texture_gt = uv_gt[:,:3,:,:]*self.uv_face_eye_mask + (uv_texture[:,:3,:,:]*(1-self.uv_face_eye_mask)*0.7)\n","        else:\n","            uv_texture_gt = uv_gt[:,:3,:,:]*self.uv_face_eye_mask + (torch.ones_like(uv_gt[:,:3,:,:])*(1-self.uv_face_eye_mask)*0.7)\n","            \n","        ## output\n","        opdict = {\n","            'vertices': verts,\n","            'normals': ops['normals'],\n","            'transformed_vertices': trans_verts,\n","            'landmarks2d': landmarks2d,\n","            'landmarks3d': landmarks3d,\n","            'uv_detail_normals': uv_detail_normals,\n","            'uv_texture_gt': uv_texture_gt,\n","            'displacement_map': uv_z+self.fixed_uv_dis[None,None,:,:],\n","        }\n","        if self.cfg.model.use_tex:\n","            opdict['albedo'] = albedo\n","            opdict['uv_texture'] = uv_texture\n","\n","        visdict = {\n","            'inputs': images, \n","            'landmarks2d': util.tensor_vis_landmarks(images, landmarks2d, isScale=False),\n","            'landmarks3d': util.tensor_vis_landmarks(images, landmarks3d, isScale=False),\n","            'shape_images': shape_images,\n","            'shape_detail_images': shape_detail_images,\n","            'uv_texture': uv_texture,\n","            'uv_texture_gt': uv_gt[:,:3,:,:]*self.uv_face_eye_mask,\n","            'uv_gt': uv_gt,\n","            'albedo': albedo\n","        }\n","        if self.cfg.model.use_tex:\n","            visdict['rendered_images'] = ops['images']\n","        return opdict, visdict\n","    \n","    @torch.no_grad()\n","    def decode_modified(self, codedict, albedo):\n","        images = codedict['images']\n","        batch_size = images.shape[0]\n","        \n","        ## decode\n","        verts, landmarks2d, landmarks3d = self.flame(shape_params=codedict['shape'], expression_params=codedict['exp'], pose_params=codedict['pose'])\n","        uv_z = self.D_detail(torch.cat([codedict['pose'][:,3:], codedict['exp'], codedict['detail']], dim=1))\n","       \n","        ## projection\n","        landmarks2d = util.batch_orth_proj(landmarks2d, codedict['cam'])[:,:,:2]; landmarks2d[:,:,1:] = -landmarks2d[:,:,1:]; landmarks2d = landmarks2d*self.image_size/2 + self.image_size/2\n","        landmarks3d = util.batch_orth_proj(landmarks3d, codedict['cam']); landmarks3d[:,:,1:] = -landmarks3d[:,:,1:]; landmarks3d = landmarks3d*self.image_size/2 + self.image_size/2\n","        trans_verts = util.batch_orth_proj(verts, codedict['cam']); trans_verts[:,:,1:] = -trans_verts[:,:,1:]\n","        \n","        ## rendering\n","        ops = self.render(verts, trans_verts, albedo, codedict['light'])\n","        uv_detail_normals = self.displacement2normal(uv_z, verts, ops['normals'])\n","        uv_shading = self.render.add_SHlight(uv_detail_normals, codedict['light'])\n","        uv_texture = albedo*uv_shading\n","\n","        landmarks3d_vis = self.visofp(ops['transformed_normals'])\n","        landmarks3d = torch.cat([landmarks3d, landmarks3d_vis], dim=2)\n","\n","        ## render shape\n","        shape_images = self.render.render_shape(verts, trans_verts, images=ops['albedo_images'], lights=codedict['light'])\n","        # shape_images = self.render.render_shape(verts, trans_verts)\n","\n","        detail_normal_images = F.grid_sample(uv_detail_normals, ops['grid'], align_corners=False)*ops['alpha_images']\n","        shape_detail_images = self.render.render_shape(verts, trans_verts, images=ops['albedo_images'], detail_normal_images=detail_normal_images, lights=codedict['light'])\n","        # shape_detail_images = self.render.render_shape(verts, trans_verts, detail_normal_images=detail_normal_images)\n","        \n","        ## extract texture\n","        ## TODO: current resolution 256x256, support higher resolution, and add visibility\n","        uv_pverts = self.render.world2uv(trans_verts)\n","        uv_gt = F.grid_sample(images, uv_pverts.permute(0,2,3,1)[:,:,:,:2], mode='bilinear')\n","        if self.cfg.model.use_tex:\n","            ## TODO: poisson blending should give better-looking results\n","            uv_texture_gt = uv_gt[:,:3,:,:]*self.uv_face_eye_mask + (uv_texture[:,:3,:,:]*(1-self.uv_face_eye_mask)*0.7)\n","        else:\n","            uv_texture_gt = uv_gt[:,:3,:,:]*self.uv_face_eye_mask + (torch.ones_like(uv_gt[:,:3,:,:])*(1-self.uv_face_eye_mask)*0.7)\n","            \n","        ## output\n","        opdict = {\n","            'vertices': verts,\n","            'normals': ops['normals'],\n","            'transformed_vertices': trans_verts,\n","            'landmarks2d': landmarks2d,\n","            'landmarks3d': landmarks3d,\n","            'uv_detail_normals': uv_detail_normals,\n","            'uv_texture_gt': uv_texture_gt,\n","            'displacement_map': uv_z+self.fixed_uv_dis[None,None,:,:],\n","        }\n","        if self.cfg.model.use_tex:\n","            opdict['albedo'] = albedo\n","            opdict['uv_texture'] = uv_texture\n","\n","        visdict = {\n","            'inputs': images, \n","            'landmarks2d': util.tensor_vis_landmarks(images, landmarks2d, isScale=False),\n","            'landmarks3d': util.tensor_vis_landmarks(images, landmarks3d, isScale=False),\n","            'shape_images': shape_images,\n","            'shape_detail_images': shape_detail_images,\n","            'uv_texture': uv_texture,\n","            'uv_texture_gt': uv_gt[:,:3,:,:]*self.uv_face_eye_mask,\n","            'uv_gt': uv_gt,\n","            'albedo': albedo,\n","            'albedo_image': ops['albedo_images'],\n","            'detail_normal_images': detail_normal_images\n","        }\n","        if self.cfg.model.use_tex:\n","            visdict['rendered_images'] = ops['images']\n","        return opdict, visdict\n","\n","    def visualize(self, visdict, size=None):\n","        grids = {}\n","        if size is None:\n","            size = self.image_size\n","        for key in visdict:\n","            grids[key] = torchvision.utils.make_grid(F.interpolate(visdict[key], [size, size])).detach().cpu()\n","        grid = torch.cat(list(grids.values()), 2)\n","        grid_image = (grid.numpy().transpose(1,2,0).copy()*255)[:,:,[2,1,0]]\n","        grid_image = np.minimum(np.maximum(grid_image, 0), 255).astype(np.uint8)\n","        return grid_image\n","    \n","    def save_obj(self, filename, opdict):\n","        '''\n","        vertices: [nv, 3], tensor\n","        texture: [3, h, w], tensor\n","        '''\n","        i = 0\n","        vertices = opdict['vertices'][i].cpu().numpy()\n","        faces = self.render.faces[0].cpu().numpy()\n","        texture = util.tensor2image(opdict['uv_texture_gt'][i])\n","        uvcoords = self.render.raw_uvcoords[0].cpu().numpy()\n","        uvfaces = self.render.uvfaces[0].cpu().numpy()\n","        # save coarse mesh, with texture and normal map\n","        normal_map = util.tensor2image(opdict['uv_detail_normals'][i]*0.5 + 0.5)\n","        util.write_obj(filename, vertices, faces, \n","                        texture=texture, \n","                        uvcoords=uvcoords, \n","                        uvfaces=uvfaces, \n","                        normal_map=normal_map)\n","        # upsample mesh, save detailed mesh\n","        texture = texture[:,:,[2,1,0]]\n","        normals = opdict['normals'][i].cpu().numpy()\n","        displacement_map = opdict['displacement_map'][i].cpu().numpy().squeeze()\n","        dense_vertices, dense_colors, dense_faces = util.upsample_mesh(vertices, normals, faces, displacement_map, texture, self.dense_template)\n","        util.write_obj(filename.replace('.obj', '_detail.obj'), \n","                        dense_vertices, \n","                        dense_faces,\n","                        colors = dense_colors,\n","                        inverse_face_order=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dB6TyxLFOh4M"},"source":["# Biofacenet"]},{"cell_type":"code","metadata":{"id":"9LYRKJekOiDx"},"source":["def scalingNet(lightingparameters,b,fmel,fblood,Shading,specmask,bSize=2):\n","  '''\n","  Inputs:\n","      lightingparameters              :   [N,15]\n","      b                               :   [N,2]  \n","      fmel,fblood,Shading,specmask    :   [N,H,W]\n","  Outputs:\n","      weightA, weightD                :   [N]\n","      CCT                             :   [N]\n","      Fweights                        :   [N,12]\n","      b, BGrid                        :   [N,2]  \n","      fmel,fblood,Shading,specmask    :   [N,H,W]\n","  '''\n","  lightingweights = F.softmax(lightingparameters[:,0:14],1) # [N,14]\n","  weightA = lightingweights[:,0] # [N]\n","  weightD = lightingweights[:,1]\n","  Fweights = lightingweights[:,2:14] # [N,12]\n","  CCT = lightingparameters[:,14] # [N]\n","  CCT = (21/(1 + torch.exp(-CCT))) # scale [0-21] since 0 indexing\n","  \n","  b = 6*torch.sigmoid(b) - 3 # Remove range constraint\n","  BGrid =  b/3.0  \n","\n","  fblood = torch.tanh(fblood)\n","  fmel = torch.tanh(fmel)\n","\n","  Shading = torch.exp(Shading)\n","  specmask = torch.exp(specmask)\n","\n","  return weightA,weightD,CCT,Fweights,b,BGrid,fmel,fblood,Shading,specmask\n","\n","\n","def cameraModel_modified(mu,PC,rgbCMF,b,wavelength):\n","    '''\n","    Inputs:\n","        mu         :  [99,1]\n","        PC         :  [99,2]\n","        b          :  [N,2] \n","    Outputs:\n","        Sr,Sg,Sb   :  [N,33]\n","    '''\n","    N = b.shape[1]\n","    # PCA model\n","    S = mu + torch.matmul(PC,b.T)  # [99,N]\n","    S =  F.relu(S) \n","    \n","    Sr = S[0:wavelength].permute(1,0)  # [33,N] -> [N,33]           \n","    Sg = S[wavelength:wavelength*2].permute(1,0)\n","    Sb = S[wavelength*2:wavelength*3].permute(1,0)\n","\n","    # S = rgbCMF\n","    # Sr = S[0, 0].unsqueeze(0).repeat(b.shape[0], 1)  # [33,N] -> [N,33]           \n","    # Sg = S[0, 1].unsqueeze(0).repeat(b.shape[0], 1)\n","    # Sb = S[0, 2].unsqueeze(0).repeat(b.shape[0], 1)\n","    # print(S[0, 0])\n","    return Sr,Sg, Sb\n","\n","def illuminationModel_modified(weightA,weightD,Fweights,CCT,illumA,illumDNorm,illumFNorm):\n","    '''\n","    Inputs:\n","        weightA, weightD                :   [N]\n","        Fweights                        :   [N,12]\n","        CCT                             :   [N]\n","        illumA                          :   [33]\n","        illumDNorm                      :   [33,22]\n","        illumFNorm                      :   [33,12]\n","    Output:\n","        e                               :   [N,33]\n","    '''\n","    N = weightA.numel()\n","    \n","# illumination A:\n","    illuminantA = weightA.view(-1,1)*illumA.view(1,-1) # [N,33] = [N,1]*[1,33]\n","    # illuminantA = illumA.view(1,-1).repeat(N, 1) # using constant illuminant for all images\n","    # print(illumA)\n","\n","# illumination D:\n","    illumDNorm = illumDNorm.view(1,1,33,22)\n","    grid = torch.zeros(N,1,33,2).to(illumDNorm.device)\n","    grid[:,0,:,0] = (2*CCT/21-1).view(N,1)\n","    grid[:,0,:,1]=torch.tensor([-1+i*(2/32) for i in range(33)]) # [N,33] <- [33]\n","    illumD = F.grid_sample(illumDNorm.expand(N,-1,-1,-1), grid, align_corners=True).squeeze() # [N,1,1,33] -> [N,33]\n","    illuminantD = weightD.view(-1,1) * illumD # [N,33] = [N,1] * [N,33]\n","\n","# illumination F:\n","    illumFNorm = illumFNorm.view(33,12,1)\n","    Fweights = Fweights.permute(1,0) # [12,N]\n","    illuminantF = illumFNorm*Fweights \n","    illuminantF = illuminantF.sum(1).permute(1,0) # [33,12,N] -> [33,N] -> [N,33]\n","\n","    e = illuminantA + illuminantD + illuminantF \n","    e = e/e.sum(1, keepdim=True)\n","\n","    return e\n","\n","\n","def computelightcolour(e,Sr,Sg,Sb):\n","  '''\n","  Inputs:\n","      Sr,Sg,Sb         : [N,33]\n","      e                : [N,33]\n","  Output:\n","      lightcolour      : [N,3]\n","  '''\n","  lightcolour = torch.stack([(Sr*e).sum(1), (Sg*e).sum(1), (Sb*e).sum(1)],1)\n","  # print('light color', lightcolour[0])\n","  return lightcolour\n","\n","\n","def computeSpecularities(specmask,lightcolour):\n","  '''\n","  Inputs:\n","      specmask         : [N,H,W]\n","      lightcolour      : [N,3]\n","  Output:\n","      Specularities    : [N,3,H,W]\n","  '''\n","  specmask = specmask.unsqueeze(1) # [N,1,H,W]\n","  Specularities = specmask*lightcolour.view(-1,3,1,1) # [N,3,H ,W] = [N,1,H,W ] * [N,3,1,1]\n","\n","  return Specularities\n","\n","\n","def BiotoSpectralRef(fmel,fblood,Newskincolour):\n","  '''\n","  Inputs:\n","      fmel,fblood      : [N,H,W]\n","      Newskincolour    : [1,33,256,256]\n","  Output:\n","      R_total          : [N,33,H,W]\n","  '''\n","  N = fmel.shape[0]\n","  BiophysicalMaps =   torch.cat([fmel.unsqueeze(1),fblood.unsqueeze(1)], dim=1) # [N,2,H,W]   \n","  BiophysicalMaps =   BiophysicalMaps.permute(0,2,3,1)  # [N,H,W,2], grid\n","  Newskincolour = Newskincolour.expand(N,-1,-1,-1) # [N,33,256,256]\n","  R_total  = F.grid_sample(Newskincolour, BiophysicalMaps, align_corners=True)\n","  # print(R_total.shape)\n","  # R_total = Newskincolour\n","  return R_total\n","\n","\n","def ImageFormation (R_total, Sr,Sg,Sb,e,Specularities,Shading):\n","  '''\n","  Inputs:\n","      R_total          : [N,33,H,W]\n","      Sr,Sg,Sb,e       : [N,33]\n","      Specularities    : [N,3,H,W]\n","      Shading          : [N,H,W]\n","  Output:\n","      rawAppearance, diffuseAlbedo : [N,3,H,W]\n","  ''' \n","  # N = e.shape[0]\n","  e = e * 3\n","  spectraRef = R_total*e.view(-1,33,1,1) # N x 33 x H x W\n","  rChannel = (spectraRef*Sr.view(-1,33,1,1)).sum(1,keepdim=True)\n","  # print('rChannel', rChannel[0])\n","  gChannel = (spectraRef*Sg.view(-1,33,1,1)).sum(1,keepdim=True)\n","  bChannel = (spectraRef*Sb.view(-1,33,1,1)).sum(1,keepdim=True) # [N,1,H,W]\n","\n","  diffuseAlbedo = torch.cat([rChannel,gChannel,bChannel], dim=1) # [N,3,H,W]\n","  ShadedDiffuse = diffuseAlbedo*(Shading.unsqueeze(1)) # [N,3,H,W] = [N,3,H,W] * [N,1,H,W]\n","  rawAppearance = ShadedDiffuse + Specularities\n","\n","  return rawAppearance, diffuseAlbedo\n","\n","\n","def WhiteBalance(rawAppearance,lightcolour):\n","  '''\n","  Inputs:\n","      rawAppearance    : [N,3,H,W]\n","      lightcolour      : [N,3]\n","  Output:\n","      ImwhiteBalanced  : [N,3,H,W]\n","  '''\n","  ImwhiteBalanced = rawAppearance/lightcolour.view(-1,3,1,1) # [N,3,H,W] = [N,3,H,W] / [N,3,1,1]\n","\n","  return ImwhiteBalanced\n","\n","\n","def findT(Tmatrix,BGrid):\n","  '''\n","  Inputs:\n","      Tmatrix          : [1,9,128,128]\n","      BGrid            : [N,2]\n","  Output:\n","      T_RAW2XYZ        : [N,9]\n","  '''\n","  N = BGrid.shape[0]\n","  T_RAW2XYZ = F.grid_sample(Tmatrix.expand(N,-1,-1,-1), BGrid.view(N,1,1,2), align_corners=True).squeeze() # [N,9,1,1] -> [N,9]\n","  # T_RAW2XYZ = torch.tensor([0.2656, 0.1325, 0.0314, 0.0064, 0.2475, -0.1098, 0.0533, -0.0374, 0.4507]).to(device)\n","  # T_RAW2XYZ = T_RAW2XYZ.expand(N, -1)\n","\n","  return T_RAW2XYZ\n","\n","\n","def fromRawTosRGB(imWB,T_RAW2XYZ,Txyzrgb):\n","  '''\n","  Inputs:\n","      imWB             : [N,3,H,W]\n","      T_RAW2XYZ        : [N,9]\n","      Txyzrgb          : [N,9]\n","  Output:\n","      T_RAW2XYZ        : [N,9]\n","  '''\n","  T_R2X = T_RAW2XYZ.view(-1,3,3,1,1) # [[1,2,3],[4,5,6],[7,8,9]]\n","  Ix = (T_R2X[:,:,0] * imWB).sum(1) # [N,3,1,1] * [N,3,H,W] -> [N,3,H,W] -> [N,H,W]\n","  Iy = (T_R2X[:,:,1] * imWB).sum(1)\n","  Iz = (T_R2X[:,:,2] * imWB).sum(1)\n","\n","  Ixyz = torch.stack([Ix,Iy,Iz],1) # [N,3,H,W]\n","\n","  R = (Txyzrgb[0,:].view(-1,1,1) * Ixyz).sum(1) # [3,1,1] * [N,3,H,W] -> [N,3,H,W] -> [N,H,W]\n","  G = (Txyzrgb[1,:].view(-1,1,1) * Ixyz).sum(1)\n","  B = (Txyzrgb[2,:].view(-1,1,1) * Ixyz).sum(1)\n","\n","  sRGBim = torch.stack([R,G,B],1) # [N,3,H,W]\n","  sRGBim =F.relu(sRGBim)\n","\n","  return sRGBim\n","\n","\n","def priorLoss(b, weight):\n","  loss = (b*b).sum()\n","  return loss*weight\n","\n","\n","def appearanceLoss(rgbim, images, muim, weight):\n","  rgb = rgbim - muim.view(-1,1,1)\n","  delta = images - rgb\n","  loss = (delta*delta).sum()\n","  return loss*weight\n","\n","\n","def sparsityLoss(Specularities, weight):\n","  #L1 sparsity loss\n","  loss = Specularities.sum()\n","  return loss*weight\n","\n","\n","def shadingLoss(actualshading, predictedShading, actualmasks, weight):\n","  '''\n","  Inputs:\n","      actualshading, predictedShading : [N,H,W]\n","      actualmasks                     : [N,H,W] \n","  '''\n","  scale = ((actualshading*predictedShading)*actualmasks).sum(2).sum(1)/((predictedShading**2)*actualmasks).sum(2).sum(1)  # [N]\n","\n","  predictedShading = predictedShading*scale.view(-1,1,1)\n","  alpha = (actualshading - predictedShading)*actualmasks\n","  loss = (alpha**2).sum()\n","  return loss*weight\n","\n","def cstretch(img, a=0., b=1.):\n","  '''\n","  contrast stretch numpy image\n","  '''\n","  if len(img.shape) == 2:\n","    c = np.min(img)\n","    d = np.max(img)\n","    img=(img-c)/(d-c)*(b-a) + a\n","    return img\n","  for i in range(3):\n","    img[:,:,i]=cstretch(img[:,:,i],a,b)\n","  return img\n","\n","  '''\n","class to load util data\n","'''\n","class Utils():\n","    def __init__(self,path,device,n_components=2):\n","        self.LightVectorSize = 15\n","        self.wavelength = 33\n","        self.bSize = 2\n","        self.blossweight = 1e-4  \n","        self.appweight = 1e-3\n","        self.Shadingweight = 1e-5 \n","        self.sparseweight = 1e-5\n","\n","        #load data\n","        # Newskincolour = io.loadmat(path+'SpectralReflectance.mat')\n","        # Newskincolour = torch.tensor(Newskincolour['SpectralReflectance']).float()\n","        Newskincolour = io.loadmat(path+'Newskincolour.mat')\n","        Newskincolour = torch.tensor(Newskincolour['Newskincolour']).float()\n","        Newskincolour = Newskincolour.permute(2,0,1).unsqueeze(0) # [1,33,256,256]\n","        self.Newskincolour = Newskincolour.to(device)\n","        \n","        Tmatrix = io.loadmat(path+'Tmatrix.mat')\n","        Tmatrix=torch.tensor(Tmatrix['Tmatrix'])\n","        Tmatrix = Tmatrix.permute(2,0,1).unsqueeze(0)\n","        self.Tmatrix = Tmatrix.to(device)\n","        \n","        illD = io.loadmat(path+'illumDmeasured.mat')\n","        illumDmeasured = torch.tensor(illD['illumDmeasured']) \n","        illumDmeasured = illumDmeasured.permute(1,0)\n","        illumDNorm = illumDmeasured/illumDmeasured.sum(0,keepdim=True) # [33,22]\n","        self.illumDNorm = illumDNorm.to(device)\n","\n","        illA=io.loadmat(path+'illumA.mat')\n","        illumA=torch.tensor(illA['illumA']).squeeze() #[1,1,33] -> [33]\n","        illumA = illumA/illumA.sum()\n","        self.illumA = illumA.to(device)\n","\n","        illF=io.loadmat(path+'illF.mat')\n","        illumF=torch.tensor(illF['illF']) #[1,33,12]\n","        illumF = illumF.squeeze() # [33,12]\n","        illumFNorm = illumF/illumF.sum(0,keepdim=True)\n","        self.illumFNorm = illumFNorm.to(device)\n","\n","        self.Txyzrgb = torch.tensor([[3.2406, -1.5372, -0.4986], \n","                        [-0.9689, 1.8758, 0.0415], \n","                        [0.0557, -0.2040, 1.057]]).to(device)\n","\n","        # self.muim = torch.tensor([0.5394,0.4184,0.3569]).to(device)\n","        self.muim = torch.tensor([135.4503,  96.9107,  74.0034]).float().to(device) / 255\n","\n","        data=io.loadmat(path+'CamPCA.mat')\n","        self.EV=torch.tensor(data['EV']).to(device)\n","        self.PC=torch.tensor(data['PC']).to(device)\n","        self.mu=torch.tensor(data['mu']).to(device)\n","       \n","\n","        rgbData = io.loadmat(path+'rgbCMF.mat')\n","        cameraSensitivityData = np.array(list(np.array(rgbData['rgbCMF'][0])))\n","        # print('cameraSensitivityData', cameraSensitivityData.shape)\n","        pca = PCA(n_components)\n","\n","        Y = np.transpose(cameraSensitivityData, (2, 0, 1))\n","\n","        for camera in range(28):\n","            for channel in range(3):\n","                # should use max but doesn't matter since white balance divides\n","                Y[camera, channel] /= np.sum(Y[camera, channel])\n","        # print('Y', Y.shape)\n","        self.rgbCMF = torch.from_numpy(Y).float().to(device)\n","\n","        Y = np.resize(Y, (28, 99))\n","\n","        pca.fit(Y)\n","\n","        pcaComponents = pca.components_ * \\\n","            np.resize(pca.explained_variance_ ** 0.5, (n_components, 1))\n","        # Done so that vector is on the same scale as matlab\n","        pcaComponents[1] *= -1\n","\n","        self.pcaMeans = torch.reshape(torch.tensor(\n","            pca.mean_), (99, 1)).float().to(device)\n","       \n","        self.pcaComponents = torch.tensor(\n","            pcaComponents).permute(1, 0).float().to(device)\n","        # self.PC = self.pcaComponents\n","        # self.mu = self.pcaMeans\n","\n","def EncoderBlock(filters, doubleconv):\n","    conv_layers = []\n","    for i in range(len(filters)):\n","        if i==0:\n","            infilters = 3\n","        else:\n","            infilters = filters[i-1]\n","\n","        conv_layers.append(\n","            nn.Sequential(\n","                nn.Conv2d(infilters, filters[i], kernel_size=3, padding=1), \n","                nn.BatchNorm2d(filters[i]),\n","                nn.ReLU()\n","            ))\n","\n","        if doubleconv:\n","            conv_layers.append(\n","                nn.Sequential(\n","                    nn.Conv2d(filters[i], filters[i], kernel_size=3, padding=1), \n","                    nn.BatchNorm2d(filters[i]),\n","                    nn.ReLU(),\n","                    nn.Conv2d(filters[i], filters[i], kernel_size=3, padding=1), \n","                    nn.BatchNorm2d(filters[i]),\n","                    nn.ReLU(),\n","                ))\n","\n","        if i<len(filters)-1:\n","            conv_layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n","\n","    return conv_layers\n","\n","def DecoderBlock(filters, doubleconv):\n","    deconv_layers = []\n","    for i in reversed(range(len(filters)-1)):\n","        deconv_layers.append(nn.ConvTranspose2d(filters[i+1], filters[i+1], kernel_size=4, stride=2, padding=1))\n","\n","        deconv_layers.append(\n","            nn.Sequential(\n","                nn.Conv2d(filters[i+1]+filters[i], filters[i], kernel_size=3, padding=1),\n","                nn.BatchNorm2d(filters[i]),\n","                nn.ReLU()\n","            ))\n","        if doubleconv:\n","            deconv_layers.append(\n","                nn.Sequential(\n","                    nn.Conv2d(filters[i], filters[i], kernel_size=3, padding=1),\n","                    nn.BatchNorm2d(filters[i]),\n","                    nn.ReLU(),\n","                    nn.Conv2d(filters[i], filters[i], kernel_size=3, padding=1),\n","                    nn.BatchNorm2d(filters[i]),\n","                    nn.ReLU()\n","                ))\n","\n","    deconv_layers.append(nn.Conv2d(filters[0], 1, kernel_size=3, padding=1))\n","\n","    return deconv_layers\n","\n","class CNN(nn.Module):\n","    def __init__(self, nclass=4, filters=[32, 64, 128, 256, 512], doubleconv=True, LightVectorSize=15,bSize=2):\n","        super(CNN, self).__init__()\n","        self.filters = filters\n","        self.nclass = nclass\n","        self.doubleconv = doubleconv\n","        self.LightVectorSize = LightVectorSize\n","        self.bSize = bSize\n","        self.fcdim = LightVectorSize + bSize\n","\n","        self.encoder = nn.Sequential(*EncoderBlock(self.filters, self.doubleconv))      \n","        self.decoders = nn.ModuleList([nn.Sequential(*DecoderBlock(self.filters, self.doubleconv)) for _ in range(self.nclass)])\n","        self.fc = nn.Sequential(\n","                        nn.Conv2d(filters[-1], filters[-1], kernel_size=4),\n","                        nn.BatchNorm2d(filters[-1]),\n","                        nn.ReLU(),\n","                        nn.Conv2d(filters[-1], filters[-1], kernel_size=1),\n","                        nn.BatchNorm2d(filters[-1]),\n","                        nn.ReLU(),\n","                        nn.Conv2d(filters[-1], self.fcdim, kernel_size=1)\n","                    )\n","\n","    def forward(self, x):\n","        conv_feats = []\n","        for m in self.encoder.children():\n","            classname = m.__class__.__name__\n","            if classname.find('MaxPool2d') != -1:\n","                conv_feats.append(x)\n","            x = m(x)\n","        \n","        for dnum, d in enumerate(self.decoders):\n","            cidx = -1\n","            xd = x \n","            for m in d.children():    \n","                xd = m(xd)            \n","                classname = m.__class__.__name__\n","                if classname.find('ConvTranspose2d') != -1:\n","                    xd=torch.cat([xd,conv_feats[cidx]],1)\n","                    cidx = cidx -1\n","\n","            if dnum==0:\n","                z=xd\n","            else:\n","                z=torch.cat([z,xd],1) # [N,C,H,W]\n","\n","        return x,z\n","\n","    def predict(self, x):\n","        y,z = self.forward(x)\n","        icpar = self.fc(y) # illumination, camera params - [N,17,1,1]\n","        lightingparameters = icpar[:,0:self.LightVectorSize].squeeze() # [N,15,1,1] -> [N,15]\n","        b = icpar[:,self.LightVectorSize:].squeeze() # [N,2,1,1] -> [N, 2] \n","\n","        fmel = z[:,0]\n","        fblood = z[:,1]\n","        Shading = z[:,2]\n","        specmask = z[:,3] # [N,H,W]\n","\n","        return lightingparameters,b,fmel,fblood,Shading,specmask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pe6cm0GmP_ud"},"source":["# Generation"]},{"cell_type":"code","metadata":{"id":"FWCvMwXAHbok"},"source":["args = {\n","    'savefolder': 'TestSamples/teaser/results',\n","    'inputpath': $Input path$\n","    'exp_path': 'TestSamples/exp',\n","    'device': 'cuda',\n","    'iscrop': True, \n","    'detector': 'fan'\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8UyZ2IeFHXlc"},"source":["savefolder = args['savefolder']\n","device = args['device']\n","os.makedirs(savefolder, exist_ok=True)\n","\n","# DECA\n","deca_cfg.model.use_tex = True\n","deca = DECA(config = deca_cfg, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IEhpYDJqe8ac"},"source":["# Biofacenet\n","u = Utils('../../util/', device)\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","model = CNN(nclass=4, filters=[16, 32, 64, 128, 256, 512, 1024], doubleconv=True, LightVectorSize=u.LightVectorSize,bSize=u.bSize).to(device)\n","model_path = '../../biofacenet_checkpoints/buptbalanced_deca_albedo_uv_map_tanh_10_12.pt'\n","\n","model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvfrHSypfmac"},"source":["# Generation"]},{"cell_type":"code","metadata":{"id":"m936PuWVDieA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TAoc6GZXOnG6"},"source":["## Generation using bidmc wave"]},{"cell_type":"code","metadata":{"id":"M4vhgVoAcliR"},"source":["params = {\n","    'num_generated_frames': 2100,\n","    'upper_range': 1.1,\n","    'lower_range': 0.9,\n","    'generated_video_path': './../bupt_balanced_1440_09_11_no_mask_11_6_test', \n","    'batch_size': 30\n","}\n","\n","Path(params['generated_video_path']).mkdir(parents=True, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxlNV4pVOKX8"},"source":["PPG_wave_dir = '/content/gdrive/My Drive/Colorless Camera/ippg_generation/Bidmc_ppg_wave'\n","session_nums = np.arange(53)\n","\n","session_nums = np.random.permutation(session_nums)\n","print(session_nums)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4GHEgP0KxqC"},"source":["m_aap =  nn.AdaptiveAvgPool2d((128, 128)) #use adaptive avg pooling to downsample image to 80x80\n","\n","start = time.time()\n","\n","with torch.no_grad():\n","  # load test images \n","  testdata = datasets.TestData(args['inputpath'], iscrop=args['iscrop'], face_detector=args['detector'])\n","  expdata = datasets.TestData(args['exp_path'], iscrop=args['iscrop'], face_detector=args['detector'])\n","  \n","  for i in range(480): \n","    try:\n","      # load image\n","      print(i)\n","\n","      cur_ppg_path = os.path.join(PPG_wave_dir, 'bidmc_' + \"{:02d}\".format(i%53+1) + '_Signals.csv')\n","      ppg_file = pd.read_csv(cur_ppg_path)\n","      target = ppg_file[' PLETH'].values\n","      target_rppg = []\n","\n","      for rppg_index in range(params['num_generated_frames']):\n","        target_rppg.append(target[int(rppg_index / 30 * 125)])\n","      target = np.array(target_rppg)\n","\n","      # lower_range = np.random.uniform(0.7, 0.95) \n","      # upper_range = 2 - lower_range\n","      lower_range = params['lower_range']\n","      upper_range = params['upper_range']\n","\n","      ratio_to_first_frame = (target - target[0]) / (np.max(target) - np.min(target)) # in the range [-1, 1]\n","      ratio_to_first_frame = lower_range + \\\n","                          (ratio_to_first_frame - (-1))/2 * (upper_range - lower_range)\n","      ratio_to_first_frame = torch.from_numpy(ratio_to_first_frame).unsqueeze(1).unsqueeze(2).float().to(device)\n","\n","      name = testdata[i]['imagename']\n","      print(name)\n","      images = testdata[i]['image'].to(device)[None,...]\n","\n","      # Show image\n","      show_img(images.cpu()[0].permute(1, 2, 0).numpy())\n","      # show_img_final(images.cpu()[0].permute(1, 2, 0).numpy(), 'image.png')\n","      images = images.repeat(params['batch_size'], 1, 1, 1)\n","      # images = m_upsample(images)\n","      # DECA fit\n","      codedict = deca.encode(images)\n","      # print(codedict['light'].shape)\n","      opdict, visdict = deca.decode(codedict) #tensor\n","      albedo_uvmap = visdict['albedo']\n","      # show_img_final(albedo_uvmap.cpu()[0].permute(1, 2, 0).numpy(), 'albedo_uv.png')\n","      show_img(albedo_uvmap.cpu()[0].permute(1, 2, 0).numpy())\n","      \n","      # Prepare Biofacenet input\n","      albedo_uvmap_biofacenet = albedo_uvmap**2.2 - u.muim.view(-1,1,1)\n","\n","      # albedo_uvmap_biofacenet = albedo_uvmap_biofacenet.repeat(64, 1, 1, 1)\n","      \n","      # Biofacenet fblood & fmel\n","      lightingparameters,b,fmel,fblood,predictedShading,specmask = model.predict(albedo_uvmap_biofacenet)\n","      # lightingparameters = lightingparameters.view(1, -1) # Add B dim\n","      # b = b.view(1, -1) # Add B dim\n","      weightA,weightD,CCT,Fweights,b,BGrid,fmel,fblood,predictedShading,specmask = scalingNet(lightingparameters,b,fmel,fblood,predictedShading,specmask,u.bSize)\n","      \n","      \n","      # Show fmel & fblood\n","      # show_img(cm.jet(0.5*(1+fmel[0]).detach().cpu().numpy()))\n","\n","      # show_img(cm.jet(0.5*(1+fblood[0]).detach().cpu().numpy()))\n","      # show_img_final(cm.jet(0.5*(1+fblood[5]).detach().cpu().numpy()), 'blood_map.png') \n","\n","      # Generate pose\n","      target_pose = 5 * torch.randn((1, 3))\n","      for k in range(params['num_generated_frames'] - 1):\n","        new_pose = target_pose[-1, :].view(1, 3) + torch.randn((1, 3))\n","        new_pose = torch.clamp(new_pose, -15, 15)\n","        target_pose = torch.cat((target_pose, new_pose), 0)\n","\n","      # Generate expression\n","      target_exp = codedict['exp'][0:1]\n","      for k in range(params['num_generated_frames'] - 1):       \n","        new_exp = target_exp[-1, :].view(1, 50) + 0.03 * torch.randn((1, 50)).cuda()\n","        target_exp = torch.cat((target_exp, new_exp), 0)\n","\n","      video_frame = []\n","\n","      for batch_num in range(target.shape[0]//params['batch_size']): #\n","        # Edit fblood & fmel TODO: Add PPG\n","        # fmel = fmel + 0.01\n","        fmel = torch.clamp(fmel, -1, 1)\n","\n","        fblood = fblood[0].unsqueeze(0).repeat(params['batch_size'], 1, 1) \\\n","                    * ratio_to_first_frame[batch_num*params['batch_size']:batch_num*params['batch_size'] + params['batch_size']].repeat(1, 256, 256)\n","        fblood = torch.clamp(fblood, -1, 1)\n","\n","        # Biofaceent reconstruct\n","        e = illuminationModel_modified(weightA,weightD,Fweights,CCT,u.illumA,u.illumDNorm,u.illumFNorm)\n","        Sr,Sg,Sb = cameraModel_modified(u.mu,u.PC,u.rgbCMF,b,u.wavelength)\n","        lightcolour = computelightcolour(e,Sr,Sg,Sb)\n","        Specularities = computeSpecularities(specmask,lightcolour)\n","        R_total = BiotoSpectralRef(fmel, fblood, u.Newskincolour)\n","        rawAppearance,diffuseAlbedo = ImageFormation(R_total, Sr,Sg,Sb,e,Specularities,predictedShading)\n","        ImwhiteBalanced = WhiteBalance(diffuseAlbedo,lightcolour)\n","        T_RAW2XYZ = findT(u.Tmatrix,BGrid)\n","        sRGBim = fromRawTosRGB(ImwhiteBalanced,T_RAW2XYZ,u.Txyzrgb)\n","        albedo_uvmap_modified = sRGBim**(1/2.2)\n","\n","        # Edit pose\n","        euler_pose = target_pose[batch_num*params['batch_size']:batch_num*params['batch_size'] + params['batch_size'], :]\n","        # print(euler_pose)\n","        global_pose = batch_euler2axis(deg2rad(euler_pose[:, :3].cuda()))\n","        codedict['pose'][:,:3] = global_pose\n","        codedict['cam'][:,:] = 0\n","        codedict['cam'][:,0] = 8\n","\n","        # Edit expression\n","        codedict['exp'] = target_exp[batch_num*params['batch_size']:batch_num*params['batch_size'] + params['batch_size'], :]\n","        # print(codedict['exp'])\n","\n","        # DECA decode\n","        _, visdict_view = deca.decode_modified(codedict, albedo_uvmap_modified)\n","        # reconstructed_img = m_aap(torch.clamp(visdict_view['rendered_images']*255, 0, 255))\n","        reconstructed_img = m_aap(torch.clamp(visdict_view['shape_detail_images']*255, 0, 255))\n","\n","        if batch_num == 0:\n","          frame_saved = reconstructed_img.cpu().permute(0, 2, 3, 1).numpy().astype(np.uint8)       \n","        else:\n","          frame_saved = np.concatenate((frame_saved, reconstructed_img.cpu().permute(0, 2, 3, 1).numpy().astype(np.uint8)), 0)\n","      \n","      end = time.time()\n","\n","      duration = end - start\n","      print(duration)\n","      # print(frame_saved.shape)\n","      # # for i in range(frame_saved.shape[0]):\n","      # #   show_img(frame_saved[i])\n","      # new_h5_name = name + '_bidmc_' + \"{:02d}\".format(i%53+1) + '_syn.h5'\n","      # hf = h5py.File(os.path.join(params['generated_video_path'], new_h5_name), 'w')\n","      # hf.create_dataset('dataset_1', data=frame_saved)\n","      # # hf.create_dataset('masks', data=original_masks)\n","      # # hf.create_dataset('masks', data=original_mask_padded)\n","      # hf.create_dataset('ppg', data=np.array(target))\n","      # hf.close()\n","      # print(new_h5_name)\n","    except:\n","        print('error')"],"execution_count":null,"outputs":[]}]}